{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 2s 17ms/step - loss: 1.1892 - accuracy: 0.4444 - val_loss: 0.8146 - val_accuracy: 0.7007\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.8194 - accuracy: 0.6612 - val_loss: 0.6472 - val_accuracy: 0.7336\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.6516 - accuracy: 0.7527 - val_loss: 0.5865 - val_accuracy: 0.7599\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.6268 - accuracy: 0.7527 - val_loss: 0.5670 - val_accuracy: 0.7697\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.5925 - accuracy: 0.7609 - val_loss: 0.5794 - val_accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.5417 - accuracy: 0.7865 - val_loss: 0.5572 - val_accuracy: 0.7730\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.5373 - accuracy: 0.7939 - val_loss: 0.5588 - val_accuracy: 0.7895\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.5761 - accuracy: 0.7683 - val_loss: 0.5008 - val_accuracy: 0.8191\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4914 - accuracy: 0.8145 - val_loss: 0.4796 - val_accuracy: 0.8158\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4138 - accuracy: 0.8376 - val_loss: 0.4442 - val_accuracy: 0.8224\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.4081 - accuracy: 0.8475 - val_loss: 0.4402 - val_accuracy: 0.8322\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.3753 - accuracy: 0.8623 - val_loss: 0.4719 - val_accuracy: 0.8125\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.3795 - accuracy: 0.8475 - val_loss: 0.4077 - val_accuracy: 0.8520\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.3704 - accuracy: 0.8516 - val_loss: 0.4305 - val_accuracy: 0.8322\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.3295 - accuracy: 0.8730 - val_loss: 0.4002 - val_accuracy: 0.8421\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.3397 - accuracy: 0.8739 - val_loss: 0.4038 - val_accuracy: 0.8487\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2953 - accuracy: 0.8763 - val_loss: 0.3654 - val_accuracy: 0.8618\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2456 - accuracy: 0.9134 - val_loss: 0.5078 - val_accuracy: 0.8191\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.2881 - accuracy: 0.8969 - val_loss: 0.4073 - val_accuracy: 0.8421\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.2252 - accuracy: 0.9167 - val_loss: 0.5103 - val_accuracy: 0.8026\n",
      "Loading image: C:/Users/USER/Downloads/71NrRvrGF2L._AC_UF1000,1000_QL80_.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Epoch 1/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2355 - accuracy: 0.9209 - val_loss: 0.2372 - val_accuracy: 0.9145\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2161 - accuracy: 0.9217 - val_loss: 0.2258 - val_accuracy: 0.9178\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1742 - accuracy: 0.9431 - val_loss: 0.2110 - val_accuracy: 0.9243\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 0.2481 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1259 - accuracy: 0.9604 - val_loss: 0.3088 - val_accuracy: 0.8717\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1172 - accuracy: 0.9695 - val_loss: 0.3237 - val_accuracy: 0.8882\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1663 - accuracy: 0.9398 - val_loss: 0.3043 - val_accuracy: 0.9079\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1361 - accuracy: 0.9538 - val_loss: 0.2855 - val_accuracy: 0.8816\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1047 - accuracy: 0.9687 - val_loss: 0.3498 - val_accuracy: 0.8947\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0776 - accuracy: 0.9753 - val_loss: 0.3518 - val_accuracy: 0.8816\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0666 - accuracy: 0.9819 - val_loss: 0.4000 - val_accuracy: 0.8783\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0770 - accuracy: 0.9720 - val_loss: 0.3836 - val_accuracy: 0.8586\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0595 - accuracy: 0.9810 - val_loss: 0.3738 - val_accuracy: 0.8849\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0335 - accuracy: 0.9918 - val_loss: 0.4090 - val_accuracy: 0.8849\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0246 - accuracy: 0.9951 - val_loss: 0.3779 - val_accuracy: 0.8980\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0206 - accuracy: 0.9951 - val_loss: 0.4075 - val_accuracy: 0.8816\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0140 - accuracy: 0.9984 - val_loss: 0.3919 - val_accuracy: 0.8947\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0102 - accuracy: 0.9984 - val_loss: 0.4053 - val_accuracy: 0.8914\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 0.8816\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4817 - val_accuracy: 0.8816\n",
      "Loading image: C:/Users/USER/Downloads/Square_watermelon.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.1668 - accuracy: 0.9530 - val_loss: 0.2108 - val_accuracy: 0.9178\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1323 - accuracy: 0.9580 - val_loss: 0.2007 - val_accuracy: 0.9276\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0951 - accuracy: 0.9678 - val_loss: 0.2238 - val_accuracy: 0.9276\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0712 - accuracy: 0.9769 - val_loss: 0.2438 - val_accuracy: 0.9079\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0991 - accuracy: 0.9736 - val_loss: 0.2098 - val_accuracy: 0.9342\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0230 - accuracy: 0.9942 - val_loss: 0.1572 - val_accuracy: 0.9441\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0146 - accuracy: 0.9975 - val_loss: 0.1536 - val_accuracy: 0.9507\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0084 - accuracy: 0.9984 - val_loss: 0.1668 - val_accuracy: 0.9441\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.1595 - val_accuracy: 0.9507\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1688 - val_accuracy: 0.9441\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1755 - val_accuracy: 0.9474\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1710 - val_accuracy: 0.9474\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1784 - val_accuracy: 0.9474\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9474\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1846 - val_accuracy: 0.9441\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9441\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 9.4531e-04 - accuracy: 1.0000 - val_loss: 0.1910 - val_accuracy: 0.9408\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 8.7006e-04 - accuracy: 1.0000 - val_loss: 0.1921 - val_accuracy: 0.9408\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 7.6688e-04 - accuracy: 1.0000 - val_loss: 0.1954 - val_accuracy: 0.9408\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 7.1643e-04 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9375\n",
      "Loading image: C:/Users/USER/Downloads/Bunch-of-bananas-67e91d5.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Loading image: C:/Users/USER/Downloads/green-bananas.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Loading image: C:/Users/USER/Downloads/images.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Loading image: C:/Users/USER/Downloads/ph-11134207-7r98u-ll3c661grav798.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Loading image: C:/Users/USER/Downloads/s-l1200.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Loading image: C:/Users/USER/Downloads/Coconut_Drink,_Pangandaran.JPG\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Loading image: C:/Users/USER/Downloads/istockphoto-1359064574-612x612.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Loading image: C:/Users/USER/Downloads/V200W-900x900.png\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Loading image: C:/Users/USER/Downloads/ph-11134207-7r98u-ll3c661grav798.jpg\n",
      "Color image displayed successfully.\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "class ImageClassifierApp:\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Image Classifier App\")\n",
    "\n",
    "        self.class_names = {'banana': 0, 'watermelon': 1, 'orange': 2, 'basketball': 3}\n",
    "        self.epochs = 20\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.model = self.build_model()\n",
    "        self.load_dataset()\n",
    "\n",
    "        self.upload_button = tk.Button(self.root, text=\"Upload Image\", command=self.upload_image)\n",
    "        self.upload_button.pack(pady=10)\n",
    "\n",
    "        self.canvas = tk.Canvas(self.root, width=300, height=300)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        self.predict_button = tk.Button(self.root, text=\"Predict Image\", command=self.predict_image)\n",
    "        self.predict_button.pack(pady=10)\n",
    "\n",
    "        self.correct_buttons = []\n",
    "        for class_name in self.class_names:\n",
    "            button = tk.Button(self.root, text=class_name.capitalize(), command=lambda class_name=class_name: self.correct_prediction(class_name))\n",
    "            self.correct_buttons.append(button)\n",
    "            button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        self.image_path = None\n",
    "        self.pil_image = None\n",
    "        self.tk_image = None\n",
    "        self.prediction_label = tk.Label(self.root, text=\"\")\n",
    "        self.prediction_label.pack(pady=10)\n",
    "\n",
    "        self.root.mainloop()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "        model.add(layers.MaxPooling2D((2,2)))\n",
    "        model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2,2)))\n",
    "        model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(4, activation='softmax'))  # 4 output classes\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def load_dataset(self):\n",
    "        dataset_dir = \"./\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        for class_name, label in self.class_names.items():\n",
    "            class_dir = os.path.join(dataset_dir, class_name)\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                # Read image using OpenCV\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    print(f\"Failed to load image: {image_path}\")\n",
    "                    continue\n",
    "                # Resize image to (32, 32)\n",
    "                image = cv2.resize(image, (32, 32))\n",
    "                # Normalize pixel values\n",
    "                image = image / 255.0\n",
    "                # Append image and label to lists\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.images = np.array(images).reshape(-1, 32, 32, 3)  # Reshape images for color\n",
    "        self.labels = np.array(labels, dtype='int32')  # Convert labels to int32\n",
    "\n",
    "        # Shuffle images and labels\n",
    "        permutation = np.random.permutation(len(images))\n",
    "        self.images = self.images[permutation]\n",
    "        self.labels = self.labels[permutation]\n",
    "        \n",
    "        # Train the model with the loaded dataset\n",
    "        self.train_model()\n",
    "\n",
    "    def upload_image(self):\n",
    "        self.image_path = filedialog.askopenfilename(initialdir=\"/\", title=\"Select Image\", filetypes=((\"Image Files\", \"*.jpg *.png\"),))\n",
    "        if self.image_path:\n",
    "            self.load_and_display_image()\n",
    "            self.predict_image()  # Remove the argument passed to predict_image\n",
    "\n",
    "    def load_and_display_image(self):\n",
    "        print(\"Loading image:\", self.image_path)\n",
    "        color_img = cv2.imread(self.image_path)  # Read image in color mode\n",
    "        if color_img is None:\n",
    "            print(\"Error: Failed to load image.\")\n",
    "            return\n",
    "        color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)  # Convert color image to RGB format\n",
    "        color_img = cv2.resize(color_img, (300, 300))  # Resize to match model input size\n",
    "        color_img = color_img / 255.0  # Normalize pixel values\n",
    "        self.pil_image = Image.fromarray((color_img * 255).astype(np.uint8))  # Display color image\n",
    "        self.tk_image = ImageTk.PhotoImage(self.pil_image)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.canvas.create_image(0, 0, anchor=tk.NW, image=self.tk_image)\n",
    "        print(\"Color image displayed successfully.\")\n",
    "\n",
    "    def predict_image(self):\n",
    "        if self.image_path:\n",
    "            img = cv2.imread(self.image_path)  # Read image in color mode\n",
    "            img = cv2.resize(img, (32, 32))  # Resize to match model input size\n",
    "            img = img / 255.0  # Normalize pixel values\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "            prediction = self.model.predict(img)\n",
    "            predicted_class_index = np.argmax(prediction)\n",
    "            predicted_class = list(self.class_names.keys())[predicted_class_index]\n",
    "            probability = prediction[0][predicted_class_index]\n",
    "            self.prediction_label.config(text=f\"Predicted Class: {predicted_class.capitalize()}, Probability: {probability:.2f}\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Please upload an image first.\")\n",
    "\n",
    "    def correct_prediction(self, correct_class):\n",
    "        if self.image_path:\n",
    "            # Get the destination folder for the correct class\n",
    "            destination_folder = os.path.join(\".\", correct_class)\n",
    "\n",
    "            # Copy the uploaded image to the destination folder\n",
    "            shutil.copy(self.image_path, os.path.join(destination_folder, os.path.basename(self.image_path)))\n",
    "            messagebox.showinfo(\"Image Copied\", f\"Image copied to {correct_class} folder.\")\n",
    "\n",
    "            # Retrain the model using the updated dataset\n",
    "            self.load_dataset()  # Reload the dataset with the newly added image\n",
    "        else:\n",
    "            messagebox.showwarning(\"No Image\", \"No image has been uploaded yet.\")\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.fit(self.images, self.labels, epochs=self.epochs, validation_split=0.2)\n",
    "        messagebox.showinfo(\"Retraining\", \"Model retrained.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = ImageClassifierApp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2487c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
